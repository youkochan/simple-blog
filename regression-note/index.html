<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Regression Note &#8211; Simple Blog</title>
<meta name="description" content="On the way.">
<meta name="keywords" content="ml, note, regression">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Regression Note">
<meta property="og:description" content="On the way.">
<meta property="og:url" content="http://blog.qingye.me/regression-note/">
<meta property="og:site_name" content="Simple Blog">





<link rel="canonical" href="http://blog.qingye.me/regression-note/">
<link href="http://blog.qingye.me/feed.xml" type="application/atom+xml" rel="alternate" title="Simple Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://blog.qingye.me/assets/css/main.css">

<!-- Webfonts -->
<!-- <link href="//fonts.useso.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->
<!-- <link href="//fonts.useso.com/css?family=monospace:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->

<meta http-equiv="cleartype" content="on">

<!-- Load MathJax -->
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} });</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Load Modernizr -->
<script src="http://blog.qingye.me/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://blog.qingye.me/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://blog.qingye.me/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://blog.qingye.me/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://blog.qingye.me/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://blog.qingye.me/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://blog.qingye.me/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post" >

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://blog.qingye.me/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://blog.qingye.me/images/avatar2.jpg" alt="Qing YE photo" class="author-photo">
					<h4>Qing YE</h4>
					<p>You can code, they can not, that is pretty damn cool.</p>
				</li>
				<li><a href="http://blog.qingye.me/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:youkochan233@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/youkochan"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://blog.qingye.me/posts/">All Posts</a></li>
				<li><a href="http://blog.qingye.me/tags/">All Tags</a></li>
			</ul>
		</li>
		
	    
	        
	        
	    <li><a href="http://geekfan.me" target="_blank">GeekFan.me</a></li>
	  
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->




<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://blog.qingye.me/regression-note/" rel="bookmark" title="Regression Note">Regression Note</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2015-07-09T10:18:15+08:00">July 09, 2015</time></span></h2>
        
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
          Reading time ~6 minutes
        </p><!-- /.entry-reading-time -->
        
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>关于机器学习，回归的笔记。</p>

<!--more-->

<h1 id="section">回归简介</h1>
<p>不同于决策树ID3针对标称型数据，回归是预测数值型的目标值。最直观的理解就是解得一个线性方程，即所谓的回归方程（regression function），其中方程的各个系数则被成为回归系数（regression weights），求这些系数的过程就是回归。以下考虑的回归都是线性回归（linear regression）。</p>

<h2 id="section-1">标准线性回归</h2>
<p>我们如何找出权重列向量 $w$ 呢？最简单的方法即找出使得误差最小的 $w$。这里的误差是指预测值与真实值的差值，使用该误差的简单累加使得正负差值相互抵消，于是我们采用该误差的平方和。于是我们的目标即最小化该误差的平方和。如下式：</p>

<script type="math/tex; mode=display">\sum_{i=1}^{m}(y_i-{x_i}w)^2</script>

<p>假设输入数据存储在矩阵 $X$ 中（m行代表样本个数，n列代表特征个数），而我们计算出的回归系数保存在列向量 $w$ 中（n行，代表每一个特征的权重，1列每个特征的权重就一个）。则我们可以使用矩阵形式表示上述最小化目标。如下式：</p>

<script type="math/tex; mode=display">(y-Xw)^T(y-Xw)</script>

<p>上式对 $w$ 求导，可得 $-2*X^T(y-Xw)$，令其等于零，解出 $w$ 如下：</p>

<script type="math/tex; mode=display">\hat{w}=(X^TX)^{-1}X^Ty</script>

<p>上方的 $\hat{w}$ 表示这是从当前训练数据估计出的 $w$ 的最优解。同时，上述求解过程中需要有矩阵求逆的过程，因此需要判断矩阵是否可以求逆。该方法也称为普通最小二乘法（ordinary least squares）。</p>

<p>代码实现如下：</p>

<div class="highlight"><pre><code class="language-py" data-lang="py"><span class="k">def</span> <span class="nf">standRegres</span><span class="p">(</span><span class="n">xArr</span><span class="p">,</span><span class="n">yArr</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    标准回归函数</span>
<span class="sd">    xArr: 每一行代表一条训练数据</span>
<span class="sd">    yArr: 每一行代表一条训练数据的真实值</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">xMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">xArr</span><span class="p">);</span> <span class="n">yMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">yArr</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">xTx</span> <span class="o">=</span> <span class="n">xMat</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">xMat</span>
    <span class="c"># linalg.det 用来计算矩阵对应的行列式的值</span>
    <span class="k">if</span> <span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">xTx</span><span class="p">)</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">print</span> <span class="s">&quot;This matrix is singular, cannot do inverse&quot;</span>
        <span class="k">return</span>
    <span class="n">ws</span> <span class="o">=</span> <span class="n">xTx</span><span class="o">.</span><span class="n">I</span> <span class="o">*</span> <span class="p">(</span><span class="n">xMat</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">yMat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ws</span></code></pre></div>

<p>观测回归结果，代码如下：</p>

<div class="highlight"><pre><code class="language-py" data-lang="py"><span class="kn">import</span> <span class="nn">regression</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c"># 从文件中获取数据</span>
<span class="n">xArr</span><span class="p">,</span> <span class="n">yArr</span> <span class="o">=</span> <span class="n">regression</span><span class="o">.</span><span class="n">loadDataSet</span><span class="p">(</span><span class="s">&#39;ex0.txt&#39;</span><span class="p">)</span>

<span class="n">ws</span> <span class="o">=</span> <span class="n">regression</span><span class="o">.</span><span class="n">standRegres</span><span class="p">(</span><span class="n">xArr</span><span class="p">,</span> <span class="n">yArr</span><span class="p">)</span>

<span class="n">xMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">xArr</span><span class="p">)</span>
<span class="n">yMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">yArr</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="c"># 这里的xMat[:,1].flatten().A[0]的目的是将数据矩阵提取出第二列然后转化为一个list</span>
<span class="c"># 作为待描的点的横座标，y同理</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xMat</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">yMat</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c"># 排列预测值，防止绘制预测线段时错乱</span>
<span class="n">xCopy</span> <span class="o">=</span> <span class="n">xMat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">xCopy</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">yHat</span> <span class="o">=</span> <span class="n">xCopy</span> <span class="o">*</span> <span class="n">ws</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xCopy</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">yHat</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></div>

<p>运行结果如下：</p>

<figure>
	<a href="/images/blog/regression-1.png">
		<img src="/images/blog/regression-1.png" alt="" />
	</a>
</figure>

<p>几乎任何一个数据集都可以通过上述方法来建模，那么如何检测模型好坏呢？可以通过计算两个序列的相关系数来判断效果的好坏。Numpy提供了corrcoef函数来计算预测值和真实值的相关性。如下，在本例中预测值与真实值之间的相关性为0.98647356。</p>

<div class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">In [12]: yHat = xMat * ws</span>

<span class="go">In [13]: corrcoef(yHat.T, yMat)</span>
<span class="go">Out[13]:</span>
<span class="go">array([[ 1.        ,  0.98647356],</span>
<span class="go">       [ 0.98647356,  1.        ]])</span></code></pre></div>

<h2 id="section-2">局部加权线性回归</h2>
<p>局部加权线性回归（Locally Weighted Linear Regression, LWLR）主要思想是给待测点附近的每一个点赋予一定的权重，然后在该权值的基础上，进行回归。该算法解出回归系数 $w$ 的形式如下：</p>

<script type="math/tex; mode=display">\hat{w}=(X^TWX)^{-1}X^TWy</script>

<p>其中 $W$ 是一个矩阵，用来给每一个数据点赋权重。</p>

<p>LWLR使用“核”来对附近的点赋予更高的权重，通常使用高斯核，高斯核对应的权重如下：</p>

<script type="math/tex; mode=display">w(i,i)=\exp\left(\frac{\left|x^{(i)-x}\right|}{-2k^2}\right)</script>

<p>根据上述公式，生成一下高斯核的权重如下图所示，假设现在预测的点为0.5，下图表示了从0到1的点在不同高斯核下的权重。从上往下k值分别为0.5，0.1以及0.01。由图可知，k=0.01时，仅有很少的点用于训练回归模型。</p>

<figure>
	<a href="/images/blog/regression-2.png">
		<img src="/images/blog/regression-2.png" alt="" />
	</a>
</figure>

<p>预测代码如下：</p>

<div class="highlight"><pre><code class="language-py" data-lang="py"><span class="k">def</span> <span class="nf">lwlr</span><span class="p">(</span><span class="n">testPoint</span><span class="p">,</span><span class="n">xArr</span><span class="p">,</span><span class="n">yArr</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    根据当前预测的点，计算每一个点的权重并得到预测值</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">xMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">xArr</span><span class="p">);</span> <span class="n">yMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">yArr</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">xMat</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">eye</span><span class="p">((</span><span class="n">m</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="c"># 对于每一个点，计算该点对于预测点的权重，并保存在权重矩阵中</span>
        <span class="n">diffMat</span> <span class="o">=</span> <span class="n">testPoint</span> <span class="o">-</span> <span class="n">xMat</span><span class="p">[</span><span class="n">j</span><span class="p">,:]</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">diffMat</span><span class="o">*</span><span class="n">diffMat</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="o">*</span><span class="n">k</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">xTx</span> <span class="o">=</span> <span class="n">xMat</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="n">xMat</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">xTx</span><span class="p">)</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">print</span> <span class="s">&quot;This matrix is singular, cannot do inverse&quot;</span>
        <span class="k">return</span>
    <span class="n">ws</span> <span class="o">=</span> <span class="n">xTx</span><span class="o">.</span><span class="n">I</span> <span class="o">*</span> <span class="p">(</span><span class="n">xMat</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="n">yMat</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">testPoint</span> <span class="o">*</span> <span class="n">ws</span>

<span class="k">def</span> <span class="nf">lwlrTest</span><span class="p">(</span><span class="n">testArr</span><span class="p">,</span><span class="n">xArr</span><span class="p">,</span><span class="n">yArr</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    根据testArr中的每一个点，获得该点的预测值</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># 预测点个数。</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">testArr</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">yHat</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">yHat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lwlr</span><span class="p">(</span><span class="n">testArr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">xArr</span><span class="p">,</span><span class="n">yArr</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">yHat</span></code></pre></div>

<p>使用上述代码以及不同的k值对同一数据集测试，结果如下：</p>

<figure>
	<a href="/images/blog/regression-3.png">
		<img src="/images/blog/regression-3.png" alt="" />
	</a>
</figure>

<p>由上述结果可知，k=1.0时，每个点的权重都比较大，拟合结果近似于简单标准回归，效果欠拟合；k=0.05时，能比较好地拟合测试数据；k=0.003时，拟合曲线与测试曲线过于接近，效果过拟合。</p>

<p>局部加权线性回归同时增加了计算量，因为对于每一个点做预测时都需要用到整个数据集。</p>

<h2 id="section-3">岭回归</h2>
<p>岭回归（Ridge Regression）是在简单线性回归的基础上，增加了一个 $\lambda\sum_{j=1}^{n}w^2$ 使得最小化目标转化为了：</p>

<script type="math/tex; mode=display">(y-Xw)^T(y-Xw)+{\lambda}w^Tw</script>

<p>上式对 $w$ 求导，并令导数为0，可得：</p>

<script type="math/tex; mode=display">\hat{w}=(X^TX+{\lambda}I)^{-1}X^Ty</script>

<p>通过引入上述惩罚项，能够减少不重要的参数，取得更好的预测效果。同时，使用岭回归可以使得在样本数比特征数还少，或者样本之间存在较大相关关系时，仍然能保证求得回归系数。在该情况下，$(X^TX)^{-1}$ 有可能无法计算。</p>

<p>代码如下：</p>

<div class="highlight"><pre><code class="language-py" data-lang="py"><span class="k">def</span> <span class="nf">ridgeRegres</span><span class="p">(</span><span class="n">xMat</span><span class="p">,</span><span class="n">yMat</span><span class="p">,</span><span class="n">lam</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">xTx</span> <span class="o">=</span> <span class="n">xMat</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">xMat</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">xTx</span> <span class="o">+</span> <span class="n">eye</span><span class="p">(</span><span class="n">shape</span><span class="p">(</span><span class="n">xMat</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">lam</span>
    <span class="k">if</span> <span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">denom</span><span class="p">)</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">print</span> <span class="s">&quot;This matrix is singular, cannot do inverse&quot;</span>
        <span class="k">return</span>
    <span class="n">ws</span> <span class="o">=</span> <span class="n">denom</span><span class="o">.</span><span class="n">I</span> <span class="o">*</span> <span class="p">(</span><span class="n">xMat</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">yMat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ws</span>
    
<span class="k">def</span> <span class="nf">ridgeTest</span><span class="p">(</span><span class="n">xArr</span><span class="p">,</span><span class="n">yArr</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    通过不同的lam值来求得不同lam值下的权重</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">xMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">xArr</span><span class="p">);</span> <span class="n">yMat</span><span class="o">=</span><span class="n">mat</span><span class="p">(</span><span class="n">yArr</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">yMean</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">yMat</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">yMat</span> <span class="o">=</span> <span class="n">yMat</span> <span class="o">-</span> <span class="n">yMean</span>
    
    <span class="c"># 标准化x矩阵的每一列，具体做法是，所有特征都减去各自的均值并除以方差。</span>
    <span class="n">xMeans</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">xMat</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>       <span class="c"># 计算每列的均值</span>
    <span class="n">xVar</span> <span class="o">=</span> <span class="n">var</span><span class="p">(</span><span class="n">xMat</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>          <span class="c"># 计算每列的方差</span>
    <span class="n">xMat</span> <span class="o">=</span> <span class="p">(</span><span class="n">xMat</span> <span class="o">-</span> <span class="n">xMeans</span><span class="p">)</span><span class="o">/</span><span class="n">xVar</span>
    <span class="n">numTestPts</span> <span class="o">=</span> <span class="mi">30</span>             <span class="c"># 测试数据组数</span>
    <span class="n">wMat</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">numTestPts</span><span class="p">,</span><span class="n">shape</span><span class="p">(</span><span class="n">xMat</span><span class="p">)[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numTestPts</span><span class="p">):</span>
        <span class="n">ws</span> <span class="o">=</span> <span class="n">ridgeRegres</span><span class="p">(</span><span class="n">xMat</span><span class="p">,</span><span class="n">yMat</span><span class="p">,</span><span class="n">exp</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">wMat</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">=</span><span class="n">ws</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">wMat</span></code></pre></div>

<p>上述测试过程的 $\lambda$ 值从 $\exp(-10)$ 到 exp(19)，运行结果如下图：</p>

<figure>
	<a href="/images/blog/regression-4.png">
		<img src="/images/blog/regression-4.png" alt="" />
	</a>
</figure>

<p>可知，$\lambda$ 较小时，和标准线性回归差别不大；$\lambda$ 较大时，各个权重系数都被缩减到0。我们可以从中间选取使得误差最小的权重。</p>

<h2 id="lasso">Lasso与前向逐步回归</h2>
<p>lasso实际是在标准线性回归的基础上添加了限制条件，约束如下</p>

<script type="math/tex; mode=display">\sum_{k=1}^{n}\left|w_k\right|\le\lambda</script>

<p>因此，在 $\lambda$ 较小时，权重系数被缩减到0。而该约束条件下计算复杂度比较高，需要用二次规划方式求解。于是用一个比较简单的方式达到近似效果，即前向逐步回归。</p>

<p>前向逐步回归算法属于一种贪心算法，思路比较简单，即一开始，所有的权重都为0，然后每一步所做的决策是对某一个权重增加或者减少一个很小的值，计算新误差，若比之前好，则使用新权值。代码如下：</p>

<div class="highlight"><pre><code class="language-py" data-lang="py"><span class="k">def</span> <span class="nf">stageWise</span><span class="p">(</span><span class="n">xArr</span><span class="p">,</span><span class="n">yArr</span><span class="p">,</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span><span class="n">numIt</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    前向逐步回归：</span>
<span class="sd">    eps：每一步的步长，步长太小会出现震荡</span>
<span class="sd">    numIt：迭代次数，迭代次数过小会出现无法稳定</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">xMat</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">xArr</span><span class="p">);</span> <span class="n">yMat</span><span class="o">=</span><span class="n">mat</span><span class="p">(</span><span class="n">yArr</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">yMean</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">yMat</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">yMat</span> <span class="o">=</span> <span class="n">yMat</span> <span class="o">-</span> <span class="n">yMean</span>
    <span class="c"># 标准化训练样本</span>
    <span class="n">xMat</span> <span class="o">=</span> <span class="n">regularize</span><span class="p">(</span><span class="n">xMat</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">shape</span><span class="p">(</span><span class="n">xMat</span><span class="p">)</span>
    <span class="n">returnMat</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">numIt</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="n">ws</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span> <span class="n">wsTest</span> <span class="o">=</span> <span class="n">ws</span><span class="o">.</span><span class="n">copy</span><span class="p">();</span> <span class="n">wsMax</span> <span class="o">=</span> <span class="n">ws</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numIt</span><span class="p">):</span>
        <span class="k">print</span> <span class="n">ws</span><span class="o">.</span><span class="n">T</span>
        <span class="c"># 首先将最小误差设为正无穷</span>
        <span class="n">lowestError</span> <span class="o">=</span> <span class="n">inf</span><span class="p">;</span> 
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c"># 增大或者减少步长</span>
            <span class="k">for</span> <span class="n">sign</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">wsTest</span> <span class="o">=</span> <span class="n">ws</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="n">wsTest</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">eps</span><span class="o">*</span><span class="n">sign</span>
                <span class="n">yTest</span> <span class="o">=</span> <span class="n">xMat</span><span class="o">*</span><span class="n">wsTest</span>
                <span class="n">rssE</span> <span class="o">=</span> <span class="n">rssError</span><span class="p">(</span><span class="n">yMat</span><span class="o">.</span><span class="n">A</span><span class="p">,</span><span class="n">yTest</span><span class="o">.</span><span class="n">A</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">rssE</span> <span class="o">&lt;</span> <span class="n">lowestError</span><span class="p">:</span>
                    <span class="n">lowestError</span> <span class="o">=</span> <span class="n">rssE</span>
                    <span class="n">wsMax</span> <span class="o">=</span> <span class="n">wsTest</span>
        <span class="n">ws</span> <span class="o">=</span> <span class="n">wsMax</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">returnMat</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">=</span><span class="n">ws</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">returnMat</span></code></pre></div>

<p>选用步长0.005，迭代1000次，结果如下：</p>

<figure>
	<a href="/images/blog/regression-5.png">
		<img src="/images/blog/regression-5.png" alt="" />
	</a>
</figure>

<p>前向逐步回归的优点也在于它能够帮助人们理解现有的模型并进行改进。当建立一个模型后，可以根据该方式来选择一些重要的特征（权重较大的特征），就有可能及时停止对无关特征的收集。</p>

<h2 id="section-4">权衡偏差与方差</h2>

<figure>
	<a href="/images/blog/regression-6.png">
		<img src="/images/blog/regression-6.png" alt="" />
	</a>
</figure>

<p>上图描述了模型复杂度与训练误差以及测试误差的关系。上面的曲线是测试误差，下面的曲线是训练误差误差。模型的复杂度可以由模型方差表示，模型方差用以下方式度量：假设从数据集中随机取出一个子集，用该模型训练得出权重，然后再随机取一个子集，再得出权重，这些权重系数之间的差异也就是模型方差的反映。</p>

<p>针对局部加权线性回归来说，从左到右体现了高斯核的核逐步减小的过程：</p>

<ul>
  <li>核越小，模型与训练数据拟合程度越大，训练偏差越小，模型方差越大，出现过拟合；</li>
  <li>核越大，模型与训练数据拟合程度越小，训练偏差越大，模型方差越小，出现欠拟合；</li>
  <li>而不管是核过大或者过小，实际的测试误差都会较大，因为不管是过拟合还是欠拟合都不能很好地反映数据的真实走向。</li>
</ul>

<p>针对缩减法（岭回归，前向逐步回归），从左到右体现了缩减从过于严厉到无缩减的过程：</p>

<ul>
  <li>缩减越严厉，特征丢失越多，训练偏差越大，模型方差越小；</li>
  <li>缩减越宽松，特征保留越多，训练偏差越小，模型方差越大；</li>
</ul>

<p>因此，需要权衡好偏差与方差，选取最适合的模型。</p>


      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://blog.qingye.me/tags/#ml" title="Pages tagged ml" class="tag"><span class="term">ml</span></a><a href="http://blog.qingye.me/tags/#note" title="Pages tagged note" class="tag"><span class="term">note</span></a><a href="http://blog.qingye.me/tags/#regression" title="Pages tagged regression" class="tag"><span class="term">regression</span></a></span>
        
        <div class="social-share">
  <ul class="socialcount socialcount-small inline-list">
    <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://blog.qingye.me/regression-note/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li>
    <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://blog.qingye.me/regression-note/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li>
    <li class="googleplus"><a href="https://plus.google.com/share?url=http://blog.qingye.me/regression-note/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li>
  </ul>
</div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="http://blog.qingye.me/shadowsocks-proxifier/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="http://blog.qingye.me/DHTSpider/" title="DHT网络磁力链接爬虫以及搜索网站搭建">DHT网络磁力链接爬虫以及搜索网站搭建</a></h3>
      <p>回家再写。先开坑。## 背景介绍DHT 网络是一种去中心化的 P2P 网络，DHT 爬虫的目的是爬取 DHT 网络种的各种资源信息，整理得出一系列 P2P 资源信息，从而可以加以利用。根据得到的多种多样的资源信息，可以建立一个搜索引擎，从而可以对 P2P 网络中的资源进行有...&hellip; <a href="http://blog.qingye.me/DHTSpider/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="http://blog.qingye.me/write-a-ehentai-ghost/" title="编写一个E绅士扒图器">编写一个E绅士扒图器</a></h4>
        <span>Published on January 15, 2016</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="http://blog.qingye.me/write-a-sublime-plugin/" title="编写一个sublime插件">编写一个sublime插件</a></h4>
        <span>Published on January 12, 2016</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2016 Qing YE. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> -->
<script src="http://blog.qingye.me/assets/js/vendor/jquery-1.9.1.min.js"></script>
<!-- <script>window.jQuery || document.write('<script src="http://blog.qingye.me/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script> -->
<script src="http://blog.qingye.me/assets/js/scripts.min.js"></script>




    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'qingye'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
	        

</body>
</html>
